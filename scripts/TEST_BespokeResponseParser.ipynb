{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python packages\n",
    "import openai\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words in the response\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text.lower())]\n",
    "\n",
    "# Initialize the lemmatizer and whitespace tokenizer\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the .csv file containing the original ChatGPT responses\n",
    "my_anns = pd.read_csv(\"../data/modelData.csv\", encoding='cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform some standardization tasks on the ChatGPT responses\n",
    "import re\n",
    "my_anns = my_anns[my_anns[\"Residential & Commerial Development\"].notna()]\n",
    "\n",
    "my_anns[\"Response\"] = [re.sub('[^a-zA-Z ]+', '', s) for s in my_anns[\"Response\"]]\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('\\d*','')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('w{3}','')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('\\s+', ' ')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace(r'\\s+[a-zA-Z]\\s+', '')\n",
    "my_anns[\"tokenizedResponse\"] = my_anns[\"Response\"].apply(lemmatize_text)\n",
    "my_anns[\"tokenizedResponse\"] = my_anns[\"tokenizedResponse\"].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "trained_features = my_anns[\"tokenizedResponse\"]\n",
    "\n",
    "trained_features = itertools.chain.from_iterable(trained_features)\n",
    "\n",
    "trained_features = sorted(set(trained_features), key = lambda s:s.lower())\n",
    "\n",
    "my_anns[\"tokenizedResponse\"] = [\" \".join(t) for t in my_anns[\"tokenizedResponse\"]]\n",
    "my_anns = my_anns[my_anns[\"Residential & Commerial Development\"].notna()]\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizedResponse = vectorizer.fit_transform(my_anns[\"tokenizedResponse\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifiers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_data = []\n",
    "t2_data = []\n",
    "t3_data = []\n",
    "t4_data = []\n",
    "t5_data = []\n",
    "t6_data = []\n",
    "t7_data = []\n",
    "t8_data = []\n",
    "t9_data = []\n",
    "t10_data = []\n",
    "t11_data = []\n",
    "t12_data = []\n",
    "t13_data = []\n",
    "\n",
    "t1_fit = []\n",
    "t2_fit = []\n",
    "t3_fit = []\n",
    "t4_fit = []\n",
    "t5_fit = []\n",
    "t6_fit = []\n",
    "t7_fit = []\n",
    "t8_fit = []\n",
    "t9_fit = []\n",
    "t10_fit = []\n",
    "t11_fit = []\n",
    "t12_fit = []\n",
    "t13_fit = []\n",
    "\n",
    "t1_f1 = []\n",
    "t2_f1 = []\n",
    "t3_f1 = []\n",
    "t4_f1 = []\n",
    "t5_f1 = []\n",
    "t6_f1 = []\n",
    "t7_f1 = []\n",
    "t8_f1 = []\n",
    "t9_f1 = []\n",
    "t10_f1 = []\n",
    "t11_f1 = []\n",
    "t12_f1 = []\n",
    "t13_f1 = []\n",
    "\n",
    "for i in range(1000):\n",
    "    # T1 - Residential and Commercial Development\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Residential & Commerial Development\"], test_size = 0.20, stratify = my_anns[\"Residential & Commerial Development\"])\n",
    "    t1_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t1_classifier = MultinomialNB()\n",
    "    t1_classifier.fit(x_train, y_train)\n",
    "    y_pred = t1_classifier.predict(x_val)\n",
    "\n",
    "    t1_fit.append(t1_classifier)\n",
    "    t1_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T2 - Agricultural and Aquacultural Development\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Agriculture & Aquaculture\"], test_size = 0.20, stratify = my_anns[\"Agriculture & Aquaculture\"])\n",
    "    t2_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t2_classifier = MultinomialNB()\n",
    "    t2_classifier.fit(x_train, y_train)\n",
    "    y_pred = t2_classifier.predict(x_val)\n",
    "\n",
    "    t2_fit.append(t2_classifier)\n",
    "    t2_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T3 - Energy Production and Mining\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Energy Producting & Mining\"], test_size = 0.20, stratify = my_anns[\"Energy Producting & Mining\"])\n",
    "    t3_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t3_classifier = MultinomialNB()\n",
    "    t3_classifier.fit(x_train, y_train)\n",
    "    y_pred = t3_classifier.predict(x_val)\n",
    "\n",
    "    t3_fit.append(t3_classifier)\n",
    "    t3_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T4 - Transportation & Service Corridors\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Transportation & Service Corridors\"], test_size = 0.20, stratify = my_anns[\"Transportation & Service Corridors\"])\n",
    "    t4_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t4_classifier = MultinomialNB()\n",
    "    t4_classifier.fit(x_train, y_train)\n",
    "    y_pred = t4_classifier.predict(x_val)\n",
    "\n",
    "    t4_fit.append(t4_classifier)\n",
    "    t4_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T5 - Biological Resource Use\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Biological Resource Use\"], test_size = 0.20, stratify = my_anns[\"Biological Resource Use\"])\n",
    "    t5_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t5_classifier = MultinomialNB()\n",
    "    t5_classifier.fit(x_train, y_train)\n",
    "    y_pred = t5_classifier.predict(x_val)\n",
    "\n",
    "    t5_fit.append(t5_classifier)\n",
    "    t5_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T6 - Human Intrusions & Disturbance\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Human Intrusions & Disturbance\"], test_size = 0.20, stratify = my_anns[\"Human Intrusions & Disturbance\"])\n",
    "    t6_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t6_classifier = MultinomialNB()\n",
    "    t6_classifier.fit(x_train, y_train)\n",
    "    y_pred = t6_classifier.predict(x_val)\n",
    "\n",
    "    t6_fit.append(t6_classifier)\n",
    "    t6_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T7 - Natural System Modifcations\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Natural System Modifcations\"], test_size = 0.20, stratify = my_anns[\"Natural System Modifcations\"])\n",
    "    t7_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t7_classifier = MultinomialNB()\n",
    "    t7_classifier.fit(x_train, y_train)\n",
    "    y_pred = t7_classifier.predict(x_val)\n",
    "\n",
    "    t7_fit.append(t7_classifier)\n",
    "    t7_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T8 - Invasive & Other Problematic Species & Genes\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Invasive & Other Problematic Species & Genes\"], test_size = 0.20, stratify = my_anns[\"Invasive & Other Problematic Species & Genes\"])\n",
    "    t8_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t8_classifier = MultinomialNB()\n",
    "    t8_classifier.fit(x_train, y_train)\n",
    "    y_pred = t8_classifier.predict(x_val)\n",
    "\n",
    "    t8_fit.append(t8_classifier)\n",
    "    t8_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T9 - Pollution\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Pollution\"], test_size = 0.20, stratify = my_anns[\"Pollution\"])\n",
    "    t9_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t9_classifier = MultinomialNB()\n",
    "    t9_classifier.fit(x_train, y_train)\n",
    "    y_pred = t9_classifier.predict(x_val)\n",
    "\n",
    "    t9_fit.append(t9_classifier)\n",
    "    t9_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T10 - Geological Events\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Geological Events\"], test_size = 0.20, stratify = my_anns[\"Geological Events\"])\n",
    "    t10_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t10_classifier = MultinomialNB()\n",
    "    t10_classifier.fit(x_train, y_train)\n",
    "    y_pred = t10_classifier.predict(x_val)\n",
    "\n",
    "    t10_fit.append(t10_classifier)\n",
    "    t10_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T11 - Climate Change & Severe Weather\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Climate Change & Severe Weather\"], test_size = 0.20, stratify = my_anns[\"Climate Change & Severe Weather\"])\n",
    "    t11_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t11_classifier = MultinomialNB()\n",
    "    t11_classifier.fit(x_train, y_train)\n",
    "    y_pred = t11_classifier.predict(x_val)\n",
    "\n",
    "    t11_fit.append(t11_classifier)\n",
    "    t11_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T12 - Limiting/Intrinsic Population Factors\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"Limiting/Intrinsic Population Factors\"], test_size = 0.20, stratify = my_anns[\"Limiting/Intrinsic Population Factors\"])\n",
    "    t12_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t12_classifier = MultinomialNB()\n",
    "    t12_classifier.fit(x_train, y_train)\n",
    "    y_pred = t12_classifier.predict(x_val)\n",
    "\n",
    "    t12_fit.append(t12_classifier)\n",
    "    t12_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "    # T13 - DISEASE?\n",
    "    x_train, x_val, y_train, y_val = train_test_split(vectorizedResponse, my_anns[\"DISEASE?\"], test_size = 0.20, stratify = my_anns[\"DISEASE?\"])\n",
    "    t13_data.append(sum(x_train))\n",
    "    y_train = [str(y) for y in y_train]\n",
    "    y_val = [str(y) for y in y_val]\n",
    "\n",
    "    t13_classifier = MultinomialNB()\n",
    "    t13_classifier.fit(x_train, y_train)\n",
    "    y_pred = t13_classifier.predict(x_val)\n",
    "\n",
    "    t13_fit.append(t13_classifier)\n",
    "    t13_f1.append(sklearn.metrics.f1_score(y_val, y_pred, pos_label=\"True\"))\n",
    "\n",
    "scores_dict = {\"t1_f1\": t1_f1, \"t2_f1\": t2_f1, \"t3_f1\": t3_f1,\n",
    "               \"t4_f1\": t4_f1, \"t5_f1\": t5_f1, \"t6_f1\": t6_f1,\n",
    "               \"t7_f1\": t7_f1, \"t8_f1\": t8_f1, \"t9_f1\": t9_f1,\n",
    "               \"t10_f1\": t10_f1, \"t11_f1\": t11_f1, \"t12_f1\": t12_f1,\n",
    "               \"t13_f1\": t13_f1}\n",
    "scores_df = pd.DataFrame(scores_dict)\n",
    "scores_df.to_csv(\"../data/f1_scores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 16  1 ...  3  0  1]\n"
     ]
    }
   ],
   "source": [
    "print(t1_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select models with over 0.7 F1 score from each model list\n",
    "t1_fit_idx = [i for i,v in enumerate(t1_f1) if v > 0.8]\n",
    "t2_fit_idx = [i for i,v in enumerate(t2_f1) if v > 0.8]\n",
    "t3_fit_idx = [i for i,v in enumerate(t3_f1) if v > 0.8]\n",
    "t4_fit_idx = [i for i,v in enumerate(t4_f1) if v > 0.8]\n",
    "t5_fit_idx = [i for i,v in enumerate(t5_f1) if v > 0.8]\n",
    "t6_fit_idx = [i for i,v in enumerate(t6_f1) if v > 0.8]\n",
    "t7_fit_idx = [i for i,v in enumerate(t7_f1) if v > 0.8]\n",
    "t8_fit_idx = [i for i,v in enumerate(t8_f1) if v > 0.8]\n",
    "t9_fit_idx = [i for i,v in enumerate(t9_f1) if v > 0.8]\n",
    "t10_fit_idx = [i for i,v in enumerate(t10_f1) if v > 0.8]\n",
    "t11_fit_idx = [i for i,v in enumerate(t11_f1) if v > 0.8]\n",
    "t12_fit_idx = [i for i,v in enumerate(t12_f1) if v > 0.8]\n",
    "t13_fit_idx = [i for i,v in enumerate(t13_f1) if v > 0.8]\n",
    "\n",
    "# Remove those indices for which the training data was duplicated\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t1_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t1_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t1_dupe_idx = [idx for sub in t1_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t2_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t2_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t2_dupe_idx = [idx for sub in t2_dupe_idx for idx in sub]\n",
    "        \n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t3_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t3_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t3_dupe_idx = [idx for sub in t3_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t4_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t4_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t4_dupe_idx = [idx for sub in t4_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t5_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t5_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t5_dupe_idx = [idx for sub in t5_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t6_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t6_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t6_dupe_idx = [idx for sub in t6_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t7_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t7_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t7_dupe_idx = [idx for sub in t7_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t8_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t8_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t8_dupe_idx = [idx for sub in t8_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t9_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t9_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t9_dupe_idx = [idx for sub in t9_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t10_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t10_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t10_dupe_idx = [idx for sub in t10_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t11_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t11_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t11_dupe_idx = [idx for sub in t11_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t12_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t12_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t12_dupe_idx = [idx for sub in t12_dupe_idx for idx in sub]\n",
    "\n",
    "count_dict = {}\n",
    "for idx, lst in enumerate(t13_data):\n",
    "    lst_tuple = tuple(lst)\n",
    "    if lst_tuple in count_dict:\n",
    "        count_dict[lst_tuple].append(idx)\n",
    "    else:\n",
    "        count_dict[lst_tuple] = [idx]\n",
    "t13_dupe_idx = [idx for idx in count_dict.values() if len(idx) > 1]\n",
    "t13_dupe_idx = [idx for sub in t13_dupe_idx for idx in sub]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilistically\n",
    "t1_preds = {}\n",
    "for i in range(len(t1_fit_idx)):\n",
    "    t1_preds[i] = t1_fit[i].predict(vectorizedResponse)\n",
    "t1_preds = pd.DataFrame.from_dict(t1_preds)\n",
    "\n",
    "t2_preds = {}\n",
    "for i in range(len(t2_fit_idx)):\n",
    "    t2_preds[i] = t2_fit[i].predict(vectorizedResponse)\n",
    "t2_preds = pd.DataFrame.from_dict(t2_preds)\n",
    "\n",
    "t3_preds = {}\n",
    "for i in range(len(t3_fit_idx)):\n",
    "    t3_preds[i] = t3_fit[i].predict(vectorizedResponse)\n",
    "t3_preds = pd.DataFrame.from_dict(t3_preds)\n",
    "\n",
    "t4_preds = {}\n",
    "for i in range(len(t4_fit_idx)):\n",
    "    t4_preds[i] = t4_fit[i].predict(vectorizedResponse)\n",
    "t4_preds = pd.DataFrame.from_dict(t4_preds)\n",
    "\n",
    "t5_preds = {}\n",
    "for i in range(len(t5_fit_idx)):\n",
    "    t5_preds[i] = t5_fit[i].predict(vectorizedResponse)\n",
    "t5_preds = pd.DataFrame.from_dict(t5_preds)\n",
    "\n",
    "t6_preds = {}\n",
    "for i in range(len(t6_fit_idx)):\n",
    "    t6_preds[i] = t6_fit[i].predict(vectorizedResponse)\n",
    "t6_preds = pd.DataFrame.from_dict(t6_preds)\n",
    "\n",
    "t7_preds = {}\n",
    "for i in range(len(t7_fit_idx)):\n",
    "    t7_preds[i] = t7_fit[i].predict(vectorizedResponse)\n",
    "t7_preds = pd.DataFrame.from_dict(t7_preds)\n",
    "\n",
    "t8_preds = {}\n",
    "for i in range(len(t8_fit_idx)):\n",
    "    t8_preds[i] = t8_fit[i].predict(vectorizedResponse)\n",
    "t8_preds = pd.DataFrame.from_dict(t8_preds)\n",
    "\n",
    "t9_preds = {}\n",
    "for i in range(len(t9_fit_idx)):\n",
    "    t9_preds[i] = t9_fit[i].predict(vectorizedResponse)\n",
    "t9_preds = pd.DataFrame.from_dict(t9_preds)\n",
    "\n",
    "t10_preds = {}\n",
    "for i in range(len(t10_fit_idx)):\n",
    "    t10_preds[i] = t10_fit[i].predict(vectorizedResponse)\n",
    "t10_preds = pd.DataFrame.from_dict(t10_preds)\n",
    "\n",
    "t11_preds = {}\n",
    "for i in range(len(t11_fit_idx)):\n",
    "    t11_preds[i] = t11_fit[i].predict(vectorizedResponse)\n",
    "t11_preds = pd.DataFrame.from_dict(t1_preds)\n",
    "\n",
    "t12_preds = {}\n",
    "for i in range(len(t12_fit_idx)):\n",
    "    t12_preds[i] = t12_fit[i].predict(vectorizedResponse)\n",
    "t12_preds = pd.DataFrame.from_dict(t12_preds)\n",
    "\n",
    "t13_preds = {}\n",
    "for i in range(len(t13_fit_idx)):\n",
    "    t13_preds[i] = t13_fit[i].predict(vectorizedResponse)\n",
    "t13_preds = pd.DataFrame.from_dict(t13_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prediction matrices to a csv file\n",
    "t1_preds.to_csv(\"../data/preds/t1_preds.csv\")\n",
    "t2_preds.to_csv(\"../data/preds/t2_preds.csv\")\n",
    "t3_preds.to_csv(\"../data/preds/t3_preds.csv\")\n",
    "t4_preds.to_csv(\"../data/preds/t4_preds.csv\")\n",
    "t5_preds.to_csv(\"../data/preds/t5_preds.csv\")\n",
    "t6_preds.to_csv(\"../data/preds/t6_preds.csv\")\n",
    "t7_preds.to_csv(\"../data/preds/t7_preds.csv\")\n",
    "t8_preds.to_csv(\"../data/preds/t8_preds.csv\")\n",
    "t9_preds.to_csv(\"../data/preds/t9_preds.csv\")\n",
    "t10_preds.to_csv(\"../data/preds/t10_preds.csv\")\n",
    "t11_preds.to_csv(\"../data/preds/t11_preds.csv\")\n",
    "t12_preds.to_csv(\"../data/preds/t12_preds.csv\")\n",
    "t13_preds.to_csv(\"../data/preds/t13_preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the models to the whole response dataset\n",
    "# Read the .csv file containing the original ChatGPT responses\n",
    "my_anns = pd.read_csv(\"../data/subsample_gpt_responses_parseReady.csv\")\n",
    "\n",
    "# Perform some standardization tasks on the ChatGPT responses\n",
    "my_anns[\"Response\"] = [re.sub('[^a-zA-Z ]+', '', s) for s in my_anns[\"Response\"]]\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('\\d*','')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('w{3}','')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace('\\s+', ' ')\n",
    "my_anns[\"Response\"] = my_anns[\"Response\"].str.replace(r'\\s+[a-zA-Z]\\s+', '')\n",
    "my_anns[\"tokenizedResponse\"] = my_anns[\"Response\"].apply(lemmatize_text)\n",
    "my_anns[\"tokenizedResponse\"] = my_anns[\"tokenizedResponse\"].apply(lambda x: [word for word in x if word not in stop_words and word in trained_features])\n",
    "my_anns[\"tokenizedResponse\"] = [\" \".join(t) for t in my_anns[\"tokenizedResponse\"]]\n",
    "\n",
    "def dummy_fun(doc):\n",
    "    return doc\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizedResponse = vectorizer.fit_transform(my_anns[\"tokenizedResponse\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the predictions\n",
    "t1_preds = {}\n",
    "for i in range(len(t1_fit_idx)):\n",
    "    t1_preds[i] = t1_fit[i].predict(vectorizedResponse)\n",
    "t1_preds = pd.DataFrame.from_dict(t1_preds)\n",
    "\n",
    "t2_preds = {}\n",
    "for i in range(len(t2_fit_idx)):\n",
    "    t2_preds[i] = t2_fit[i].predict(vectorizedResponse)\n",
    "t2_preds = pd.DataFrame.from_dict(t2_preds)\n",
    "\n",
    "t3_preds = {}\n",
    "for i in range(len(t3_fit_idx)):\n",
    "    t3_preds[i] = t3_fit[i].predict(vectorizedResponse)\n",
    "t3_preds = pd.DataFrame.from_dict(t3_preds)\n",
    "\n",
    "t4_preds = {}\n",
    "for i in range(len(t4_fit_idx)):\n",
    "    t4_preds[i] = t4_fit[i].predict(vectorizedResponse)\n",
    "t4_preds = pd.DataFrame.from_dict(t4_preds)\n",
    "\n",
    "t5_preds = {}\n",
    "for i in range(len(t5_fit_idx)):\n",
    "    t5_preds[i] = t5_fit[i].predict(vectorizedResponse)\n",
    "t5_preds = pd.DataFrame.from_dict(t5_preds)\n",
    "\n",
    "t6_preds = {}\n",
    "for i in range(len(t6_fit_idx)):\n",
    "    t6_preds[i] = t6_fit[i].predict(vectorizedResponse)\n",
    "t6_preds = pd.DataFrame.from_dict(t6_preds)\n",
    "\n",
    "t7_preds = {}\n",
    "for i in range(len(t7_fit_idx)):\n",
    "    t7_preds[i] = t7_fit[i].predict(vectorizedResponse)\n",
    "t7_preds = pd.DataFrame.from_dict(t7_preds)\n",
    "\n",
    "t8_preds = {}\n",
    "for i in range(len(t8_fit_idx)):\n",
    "    t8_preds[i] = t8_fit[i].predict(vectorizedResponse)\n",
    "t8_preds = pd.DataFrame.from_dict(t8_preds)\n",
    "\n",
    "t9_preds = {}\n",
    "for i in range(len(t9_fit_idx)):\n",
    "    t9_preds[i] = t9_fit[i].predict(vectorizedResponse)\n",
    "t9_preds = pd.DataFrame.from_dict(t9_preds)\n",
    "\n",
    "t10_preds = {}\n",
    "for i in range(len(t10_fit_idx)):\n",
    "    t10_preds[i] = t10_fit[i].predict(vectorizedResponse)\n",
    "t10_preds = pd.DataFrame.from_dict(t10_preds)\n",
    "\n",
    "t11_preds = {}\n",
    "for i in range(len(t11_fit_idx)):\n",
    "    t11_preds[i] = t11_fit[i].predict(vectorizedResponse)\n",
    "t11_preds = pd.DataFrame.from_dict(t1_preds)\n",
    "\n",
    "t12_preds = {}\n",
    "for i in range(len(t12_fit_idx)):\n",
    "    t12_preds[i] = t12_fit[i].predict(vectorizedResponse)\n",
    "t12_preds = pd.DataFrame.from_dict(t12_preds)\n",
    "\n",
    "t13_preds = {}\n",
    "for i in range(len(t13_fit_idx)):\n",
    "    t13_preds[i] = t13_fit[i].predict(vectorizedResponse)\n",
    "t13_preds = pd.DataFrame.from_dict(t13_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the prediction matrices to a csv file\n",
    "t1_preds.to_csv(\"../data/preds/t1_preds_all.csv\")\n",
    "t2_preds.to_csv(\"../data/preds/t2_preds_all.csv\")\n",
    "t3_preds.to_csv(\"../data/preds/t3_preds_all.csv\")\n",
    "t4_preds.to_csv(\"../data/preds/t4_preds_all.csv\")\n",
    "t5_preds.to_csv(\"../data/preds/t5_preds_all.csv\")\n",
    "t6_preds.to_csv(\"../data/preds/t6_preds_all.csv\")\n",
    "t7_preds.to_csv(\"../data/preds/t7_preds_all.csv\")\n",
    "t8_preds.to_csv(\"../data/preds/t8_preds_all.csv\")\n",
    "t9_preds.to_csv(\"../data/preds/t9_preds_all.csv\")\n",
    "t10_preds.to_csv(\"../data/preds/t10_preds_all.csv\")\n",
    "t11_preds.to_csv(\"../data/preds/t11_preds_all.csv\")\n",
    "t12_preds.to_csv(\"../data/preds/t12_preds_all.csv\")\n",
    "t13_preds.to_csv(\"../data/preds/t13_preds_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save off the final annotation data\n",
    "my_anns.to_csv(\"../data/preds/final_anns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

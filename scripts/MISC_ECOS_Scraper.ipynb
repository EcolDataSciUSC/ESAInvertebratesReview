{"cells":[{"cell_type":"markdown","metadata":{"id":"1v89Wneu0LjF"},"source":["This script will navigate to the ECOS website, access links from a list of invertebrate species, determine if any 5-year reviews have been conducted for those species, and then scrape the recommended actions from each 5-year review PDF file for further analysis."]},{"cell_type":"code","execution_count":210,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50263,"status":"ok","timestamp":1701963306618,"user":{"displayName":"Vaughn","userId":"03422163621256445463"},"user_tz":300},"id":"k5Van8ul1Rmz","outputId":"43d6c5fe-d9b8-4c4f-8fa0-7819c511c014"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: PyPDF2 in d:\\anaconda3\\lib\\site-packages (3.0.1)\n","Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (2.31.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests) (2023.11.17)\n","Requirement already satisfied: bs4 in d:\\anaconda3\\lib\\site-packages (0.0.1)\n","Requirement already satisfied: beautifulsoup4 in d:\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n","Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n","Requirement already satisfied: selenium in d:\\anaconda3\\lib\\site-packages (4.16.0)\n","Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n","Requirement already satisfied: trio~=0.17 in d:\\anaconda3\\lib\\site-packages (from selenium) (0.23.1)\n","Requirement already satisfied: trio-websocket~=0.9 in d:\\anaconda3\\lib\\site-packages (from selenium) (0.11.1)\n","Requirement already satisfied: certifi>=2021.10.8 in d:\\anaconda3\\lib\\site-packages (from selenium) (2023.11.17)\n","Requirement already satisfied: attrs>=20.1.0 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n","Requirement already satisfied: sortedcontainers in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n","Requirement already satisfied: outcome in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n","Requirement already satisfied: cffi>=1.14 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n","Requirement already satisfied: wsproto>=0.14 in d:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in d:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n","Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n","Requirement already satisfied: h11<1,>=0.9.0 in d:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n","Requirement already satisfied: undetected-chromedriver in d:\\anaconda3\\lib\\site-packages (3.5.4)\n","Requirement already satisfied: selenium>=4.9.0 in d:\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (4.16.0)\n","Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (2.31.0)\n","Requirement already satisfied: websockets in d:\\anaconda3\\lib\\site-packages (from undetected-chromedriver) (12.0)\n","Requirement already satisfied: urllib3[socks]<3,>=1.26 in d:\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (1.26.16)\n","Requirement already satisfied: trio~=0.17 in d:\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.23.1)\n","Requirement already satisfied: trio-websocket~=0.9 in d:\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (0.11.1)\n","Requirement already satisfied: certifi>=2021.10.8 in d:\\anaconda3\\lib\\site-packages (from selenium>=4.9.0->undetected-chromedriver) (2023.11.17)\n","Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests->undetected-chromedriver) (3.4)\n","Requirement already satisfied: attrs>=20.1.0 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (22.1.0)\n","Requirement already satisfied: sortedcontainers in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.4.0)\n","Requirement already satisfied: outcome in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0.post0)\n","Requirement already satisfied: sniffio>=1.3.0 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.3.0)\n","Requirement already satisfied: cffi>=1.14 in d:\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (1.15.1)\n","Requirement already satisfied: wsproto>=0.14 in d:\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (1.2.0)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in d:\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.9.0->undetected-chromedriver) (1.7.1)\n","Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium>=4.9.0->undetected-chromedriver) (2.21)\n","Requirement already satisfied: h11<1,>=0.9.0 in d:\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.9.0->undetected-chromedriver) (0.14.0)\n","Collecting pdfplumber\n","  Obtaining dependency information for pdfplumber from https://files.pythonhosted.org/packages/f8/d3/f58c2d5d86a585e438c6708f568eca79e7c4e6ee3d5210cf8b31d38cb021/pdfplumber-0.10.3-py3-none-any.whl.metadata\n","  Downloading pdfplumber-0.10.3-py3-none-any.whl.metadata (38 kB)\n","Collecting pdfminer.six==20221105 (from pdfplumber)\n","  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n","     ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n","     ---------------------------------------- 0.0/5.6 MB 960.0 kB/s eta 0:00:06\n","     ---------------------------------------- 0.1/5.6 MB 1.1 MB/s eta 0:00:06\n","      --------------------------------------- 0.1/5.6 MB 1.0 MB/s eta 0:00:06\n","     - -------------------------------------- 0.2/5.6 MB 1.3 MB/s eta 0:00:05\n","     - -------------------------------------- 0.2/5.6 MB 1.3 MB/s eta 0:00:05\n","     -- ------------------------------------- 0.3/5.6 MB 1.2 MB/s eta 0:00:05\n","     -- ------------------------------------- 0.3/5.6 MB 1.1 MB/s eta 0:00:05\n","     --- ------------------------------------ 0.5/5.6 MB 1.3 MB/s eta 0:00:04\n","     ---- ----------------------------------- 0.7/5.6 MB 1.6 MB/s eta 0:00:04\n","     ----- ---------------------------------- 0.8/5.6 MB 1.8 MB/s eta 0:00:03\n","     ------ --------------------------------- 0.9/5.6 MB 1.9 MB/s eta 0:00:03\n","     ------ --------------------------------- 1.0/5.6 MB 1.8 MB/s eta 0:00:03\n","     -------- ------------------------------- 1.2/5.6 MB 2.1 MB/s eta 0:00:03\n","     ---------- ----------------------------- 1.5/5.6 MB 2.3 MB/s eta 0:00:02\n","     ----------- ---------------------------- 1.6/5.6 MB 2.3 MB/s eta 0:00:02\n","     ------------ --------------------------- 1.8/5.6 MB 2.4 MB/s eta 0:00:02\n","     -------------- ------------------------- 2.1/5.6 MB 2.6 MB/s eta 0:00:02\n","     --------------- ------------------------ 2.2/5.6 MB 2.6 MB/s eta 0:00:02\n","     ----------------- ---------------------- 2.5/5.6 MB 2.8 MB/s eta 0:00:02\n","     ------------------ --------------------- 2.6/5.6 MB 2.9 MB/s eta 0:00:02\n","     ------------------- -------------------- 2.7/5.6 MB 2.8 MB/s eta 0:00:02\n","     ------------------- -------------------- 2.7/5.6 MB 2.6 MB/s eta 0:00:02\n","     -------------------- ------------------- 2.9/5.6 MB 2.7 MB/s eta 0:00:01\n","     --------------------- ------------------ 3.0/5.6 MB 2.7 MB/s eta 0:00:01\n","     ---------------------- ----------------- 3.2/5.6 MB 2.7 MB/s eta 0:00:01\n","     ----------------------- ---------------- 3.3/5.6 MB 2.7 MB/s eta 0:00:01\n","     ------------------------ --------------- 3.5/5.6 MB 2.8 MB/s eta 0:00:01\n","     -------------------------- ------------- 3.7/5.6 MB 2.8 MB/s eta 0:00:01\n","     ---------------------------- ----------- 4.1/5.6 MB 3.0 MB/s eta 0:00:01\n","     ----------------------------- ---------- 4.1/5.6 MB 3.0 MB/s eta 0:00:01\n","     ------------------------------ --------- 4.3/5.6 MB 2.9 MB/s eta 0:00:01\n","     -------------------------------- ------- 4.6/5.6 MB 3.1 MB/s eta 0:00:01\n","     ----------------------------------- ---- 4.9/5.6 MB 3.2 MB/s eta 0:00:01\n","     ------------------------------------ --- 5.1/5.6 MB 3.2 MB/s eta 0:00:01\n","     ------------------------------------- -- 5.3/5.6 MB 3.2 MB/s eta 0:00:01\n","     ---------------------------------------  5.6/5.6 MB 3.3 MB/s eta 0:00:01\n","     ---------------------------------------- 5.6/5.6 MB 3.3 MB/s eta 0:00:00\n","Requirement already satisfied: Pillow>=9.1 in d:\\anaconda3\\lib\\site-packages (from pdfplumber) (10.0.1)\n","Collecting pypdfium2>=4.18.0 (from pdfplumber)\n","  Obtaining dependency information for pypdfium2>=4.18.0 from https://files.pythonhosted.org/packages/16/f2/69b0609f7d8cb7ed01a27eda361674243e6097377c4610aca704a65fdf66/pypdfium2-4.25.0-py3-none-win_amd64.whl.metadata\n","  Downloading pypdfium2-4.25.0-py3-none-win_amd64.whl.metadata (47 kB)\n","     ---------------------------------------- 0.0/47.8 kB ? eta -:--:--\n","     ---------------------------------------- 47.8/47.8 kB ? eta 0:00:00\n","Requirement already satisfied: charset-normalizer>=2.0.0 in d:\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (2.0.4)\n","Requirement already satisfied: cryptography>=36.0.0 in d:\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (41.0.3)\n","Requirement already satisfied: cffi>=1.12 in d:\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n","Requirement already satisfied: pycparser in d:\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n","Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n","   ---------------------------------------- 0.0/49.0 kB ? eta -:--:--\n","   ---------------------------------------- 49.0/49.0 kB ? eta 0:00:00\n","Downloading pypdfium2-4.25.0-py3-none-win_amd64.whl (2.7 MB)\n","   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n","   ---- ----------------------------------- 0.3/2.7 MB 6.3 MB/s eta 0:00:01\n","   ----- ---------------------------------- 0.4/2.7 MB 4.2 MB/s eta 0:00:01\n","   --------- ------------------------------ 0.7/2.7 MB 5.3 MB/s eta 0:00:01\n","   -------------- ------------------------- 1.0/2.7 MB 5.3 MB/s eta 0:00:01\n","   ---------------- ----------------------- 1.2/2.7 MB 5.2 MB/s eta 0:00:01\n","   -------------------- ------------------- 1.4/2.7 MB 4.8 MB/s eta 0:00:01\n","   --------------------- ------------------ 1.5/2.7 MB 4.6 MB/s eta 0:00:01\n","   ------------------------ --------------- 1.7/2.7 MB 4.4 MB/s eta 0:00:01\n","   -------------------------- ------------- 1.8/2.7 MB 4.3 MB/s eta 0:00:01\n","   -------------------------------- ------- 2.2/2.7 MB 4.7 MB/s eta 0:00:01\n","   ----------------------------------- ---- 2.4/2.7 MB 4.7 MB/s eta 0:00:01\n","   ---------------------------------------  2.7/2.7 MB 4.8 MB/s eta 0:00:01\n","   ---------------------------------------- 2.7/2.7 MB 4.5 MB/s eta 0:00:00\n","Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n","Successfully installed pdfminer.six-20221105 pdfplumber-0.10.3 pypdfium2-4.25.0\n"]}],"source":["# Install libraries\n","!pip install PyPDF2\n","!pip install requests\n","!pip install bs4\n","!pip install selenium\n","!pip install undetected-chromedriver\n","!pip install pdfplumber"]},{"cell_type":"code","execution_count":211,"metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1701963328074,"user":{"displayName":"Vaughn","userId":"03422163621256445463"},"user_tz":300},"id":"F11h2GW70aiU"},"outputs":[],"source":["# Load libraries\n","import pdfplumber\n","from bs4 import BeautifulSoup\n","from functools import reduce\n","from selenium import webdriver\n","from selenium.webdriver.chrome.service import Service\n","from selenium.webdriver.support.ui import WebDriverWait\n","from selenium.webdriver.support import expected_conditions as EC\n","from selenium.webdriver.support.ui import Select\n","from selenium.webdriver.common.by import By\n","\n","from lxml import html\n","\n","import pandas as pd\n","import requests\n","import re\n","import io\n","import undetected_chromedriver as uc\n","import time\n"]},{"cell_type":"code","execution_count":91,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1701963329234,"user":{"displayName":"Vaughn","userId":"03422163621256445463"},"user_tz":300},"id":"Higde7t_z8mA"},"outputs":[],"source":["# URL to the index of ESA-listed species:\n","CUSTOM_QUERY_URL = \"https://ecos.fws.gov/ecp0/reports/ad-hoc-species-report-input\"\n","CUSTOM_JS_QUERY = \"kingdom\"\n","CUSTOM_OPTION_VALUE = \"Invertebrates\"\n","CUSTOM_SUBMIT_QUERY = \"submit\"\n","\n","SP_BASE_URL = \"https://ecos.fws.gov\""]},{"cell_type":"code","execution_count":102,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":774},"executionInfo":{"elapsed":1082,"status":"error","timestamp":1701963331429,"user":{"displayName":"Vaughn","userId":"03422163621256445463"},"user_tz":300},"id":"wudWMTG-2zdU","outputId":"962e5262-af46-4172-8cb0-bcb96fe21579"},"outputs":[],"source":["# Open the webpage with Selenium to access all ESA-listed animals\n","driver = webdriver.Chrome()\n","driver.get(CUSTOM_QUERY_URL)\n","\n","# Save the current tab handle to switch to new query results tab\n","original_window = driver.current_window_handle\n","\n","# Select the report length dropdown element and show all species on page\n","select = Select(driver.find_element(By.NAME, CUSTOM_JS_QUERY))\n","select.deselect_all()\n","select.select_by_visible_text(CUSTOM_OPTION_VALUE)\n","driver.find_element(By.ID, CUSTOM_SUBMIT_QUERY).submit()\n","time.sleep(5)\n","\n","# Grab the newly visible HTML on the webpage\n","for window_handle in driver.window_handles:\n","    if window_handle != original_window:\n","        driver.switch_to.window(window_handle)\n","        break\n","    \n","webpage = driver.page_source"]},{"cell_type":"code","execution_count":103,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":295,"status":"ok","timestamp":1701962179531,"user":{"displayName":"Vaughn","userId":"03422163621256445463"},"user_tz":300},"id":"8lu5ggsy1CgD","outputId":"58d8acf4-b1ef-4a61-c08b-0dfe91e604d1"},"outputs":[],"source":["# Grab the HTMl representation of the page\n","soup = BeautifulSoup(webpage)"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["# Find HTML links to species pages and save them to a list, append the base URL to all species links\n","links = [a.get('href') for a in soup.find_all('a', href=True)]\n","sp_page_links = [i for i in links if \"species\" in i]\n","sp_page_links = [SP_BASE_URL + i for i in sp_page_links]"]},{"cell_type":"markdown","metadata":{},"source":["## Species Drill Down ##\n","Now we will navigate into individual species pages, parse out relevant traits like listing year, current status, review reports, etc. We will also save the URLs to the PDF files of the review reports to drill down even further."]},{"cell_type":"code","execution_count":163,"metadata":{},"outputs":[],"source":["# Some notes on species page parsing just to keep things a bit organized:\n","# Scientific Name = italic text in div ID 'speciesProfile' >> 'j-species-name'\n","# Listing Status = id is 'listingStatus, text in span 'listingEnd'\n","# Listing Location = visual text, text in span 'endangerStatus' \n","# Listing Years = div id 'j-listing-status-summary', table id 'DataTables_Table_0', table body, 2nd <td> tags\n","# Five-year Review Years = div id 'j-five-year-reviews', table id 'DataTables_Table_4', table body, 1st <td> tags\n","# Five-year Review URLS = div id 'j-five-year-reviews', table id 'DataTables_Table_4', table body, 2nd column <a> tags\n","summary_dataframe = pd.DataFrame()\n","\n","# Ensure that selenium is operating silently to prevent annoying driver pop-ups\n","from selenium.webdriver.chrome.options import Options\n","chrome_options = Options()\n","chrome_options.add_argument(\"--headless\")\n","\n","for i in sp_page_links:\n","    driver = webdriver.Chrome(options = chrome_options)\n","\n","    # Try to get the species profile page, except the timeout error if page is wonky. Then move to the next.\n","    try:\n","        driver.get(i)\n","    except TimeoutError:\n","        pass\n","\n","    time.sleep(2)\n","\n","    webpage = driver.page_source\n","    soup = BeautifulSoup(webpage)\n","\n","    taxon_id = re.search('(\\d+)$', i).group(1)\n","    scientific_name = soup.find('div', {'id': 'speciesProfile'}).find('i').get_text()\n","    listing_status = soup.find('div', {'id': 'speciesProfile'}).find('span', {'id': 'listingStatus'}).get_text().replace(\"Listing Status: \", \"\")\n","    \n","    # Listing Status\n","    listingStatusPresent = soup.find('div', {'id': 'speciesProfile'}).find('span', {'class': 'endangerStatus'}) is not None\n","    if listingStatusPresent:\n","        listing_location = soup.find('div', {'id': 'speciesProfile'}).find('span', {'class': 'endangerStatus'}).get_text().capitalize()\n","    else:\n","        listing_location = \"\"\n","\n","    # Listing Dates\n","    listingDatesPresent = soup.find('table', {'id': 'DataTables_Table_0'}) is not None\n","    if listingDatesPresent:\n","        listing_dates = []\n","        for row in soup.find('table', {'id': 'DataTables_Table_0'}).tbody.find_all('tr'):\n","            listing_dates.append(row.find_all('td')[1].text)\n","        listing_dates = \"; \".join([str(x) for x in listing_dates])\n","    else:\n","        listing_dates = \"\"\n","\n","    # 5-year Review Dates\n","    reviewDatesTablePresent = soup.find('div', {'id': 'j-five-year-reviews'}).find('tbody') is not None\n","    if reviewDatesTablePresent:\n","        review_dates = []\n","        for row in soup.find('div', {'id': 'j-five-year-reviews'}).find('tbody').find_all('tr'):\n","            review_dates.append(row.find_all('td')[0].text.replace(\"/\", \"-\"))\n","        review_dates = \"; \".join([str(x) for x in review_dates])\n","    else:\n","        review_dates = \"\"\n","\n","    # 5-year Review URLs\n","    reviewURLsTablePresent = soup.find('div', {'id': 'j-five-year-reviews'}).find('tbody') is not None\n","    if reviewURLsTablePresent:\n","        review_URLs = []\n","        for row in soup.find('div', {'id': 'j-five-year-reviews'}).find('tbody').find_all('tr'):\n","            review_URLs.append(row.find_all('td')[1].find_all('a', href=True)[0].get('href'))\n","        review_URLs = \"; \".join([str(x) for x in review_URLs])\n","    else:\n","        review_URLs = \"\"\n","\n","    df2 = pd.DataFrame([[taxon_id, scientific_name, listing_status, listing_location, listing_dates, review_dates, review_URLs]],\n","    columns = ['taxon_id', \"scientificName\", \"currentListingStatus\", \"currentListingLocality\", \"allListingDates\", \"allReviewDates\", \"allReviewURLs\"])\n","\n","    summary_dataframe = pd.concat([summary_dataframe, df2])"]},{"cell_type":"code","execution_count":164,"metadata":{},"outputs":[],"source":["# Write the summary dataframe to a .csv file\n","summary_dataframe.to_csv(\"../data/summary_table.csv\")"]},{"cell_type":"code","execution_count":170,"metadata":{},"outputs":[],"source":["# Pull a random sample of PDF review documents, parse into sentences, and save those sentences to a .csv\n","# for manual annotation\n","summary_dataframe_linked = summary_dataframe[summary_dataframe['allReviewURLs'] != \"\"]\n","summary_dataframe_linked = summary_dataframe_linked.sample(n = 50, random_state=0)"]},{"cell_type":"code","execution_count":179,"metadata":{},"outputs":[],"source":["# Parse the URLs randomly\n","summary_dataframe_linked['parsedURL'] = summary_dataframe_linked['allReviewURLs'].str.partition(\";\")[0]"]},{"cell_type":"code","execution_count":213,"metadata":{},"outputs":[],"source":["# Parse the text from PDFs into a list\n","output = []\n","for i in summary_dataframe_linked['parsedURL']:\n","    request = requests.get(i)\n","    f = io.BytesIO(request.content)\n","    reader = pdfplumber.open(f)\n","\n","    text = \"\"\n","    npages = len(reader.pages)\n","    for j in range(npages):\n","        page = reader.pages[j]\n","        text += page.extract_text()\n","    \n","    output.append(text)\n","    reader.close()"]},{"cell_type":"code","execution_count":225,"metadata":{},"outputs":[],"source":["# Parse the list of text into list of sentences (list of lists)\n","from nltk.tokenize import punkt\n","import nltk \n","import string\n","\n","sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n","\n","output_sentences = []\n","for i in output:\n","    i = re.sub('[^A-Za-z0-9.]+', ' ', i)\n","    sentence = sent_detector.tokenize(i.strip().replace(\"\\n\", \" \"))\n","    output_sentences.append(sentence)\n","\n","output_sentences = [i for x in output_sentences for i in x if len(i) >= 50]\n","output_sentences = [x.strip().translate(str.maketrans('', '', string.punctuation)) for x in output_sentences]"]},{"cell_type":"code","execution_count":226,"metadata":{},"outputs":[],"source":["# Randomly select 1,000 sentences from the flattened list of lists\n","from random import sample\n","import csv\n","\n","output_sentences_sample = sample(output_sentences, 1500)\n","with open(\"../data/sentenceSample.csv\", 'w') as myfile:\n","    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL, dialect='excel')\n","    wr.writerow(output_sentences_sample)"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMnqUQ4oa5PHYHd6VUpitbI","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":0}

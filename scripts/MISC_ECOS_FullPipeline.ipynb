{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Custom functions ####\n",
    "# Remove proper nouns\n",
    "def remove_proper(data):\n",
    "    tagged_sentence = nltk.tag.pos_tag(data.split())\n",
    "    edited_sentence = [word for word, tag in tagged_sentence if tag != \"NNP\" and tag != \"NNPS\"]\n",
    "    edited_sentence = ' '.join(edited_sentence)\n",
    "    return edited_sentence\n",
    "\n",
    "# Remove non-English words\n",
    "def remove_nonEnglish(data):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    original_sentence = data.split()\n",
    "    edited_sentence = [word for word in original_sentence if word in words]\n",
    "    edited_sentence = ' '.join(edited_sentence)\n",
    "    return edited_sentence\n",
    "\n",
    "# Remove stop words\n",
    "def remove_stopWords(data):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    original_sentence = data.split()\n",
    "    edited_sentence = [word for word in original_sentence if word not in stop_words]\n",
    "    edited_sentence = ' '.join(edited_sentence)\n",
    "    return edited_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the summary CSV file which contains species and review information\n",
    "my_sp = pd.read_csv(\"../data/summary_table.csv\")\n",
    "my_sp.head(10)\n",
    "\n",
    "my_sp = my_sp[my_sp[\"allReviewURLs\"].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "for idx, row in my_sp.iterrows():\n",
    "\n",
    "    # Split the string of URLs to a list of each URL\n",
    "    my_urls = row[\"allReviewURLs\"].split(\";\")\n",
    "\n",
    "    # Iterate over the list of URLS\n",
    "    all_outputs = []\n",
    "    for url in my_urls:\n",
    "        # Save any natural language to a list of sentences.\n",
    "        output = []\n",
    "\n",
    "        # Confirm that the list element is actually a URL to a PDF file, otherwise skip it\n",
    "        if \"pdf\" in url:\n",
    "            request = requests.get(url)\n",
    "            try:\n",
    "                text = \"\"\n",
    "                npages = len(reader.pages)\n",
    "                f = io.BytesIO(request.content)\n",
    "                reader = pdfplumber.open(f)\n",
    "\n",
    "                for j in range(npages):\n",
    "                    page = reader.pages[j]\n",
    "                    try:\n",
    "                        text += page.extract_text().apply(lambda x: remove_nonEnglish(x)).apply(lambda x: remove_proper(x)).apply(lambda x: re.sub(r'[0-9]','', x)).apply(lambda x: x.lower())\n",
    "                    except:\n",
    "                        pass\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            output.append(text)\n",
    "            reader.close()\n",
    "\n",
    "        # Otherwise, skip that URL    \n",
    "        else:\n",
    "            output = []\n",
    "    \n",
    "        all_outputs.append(output)\n",
    "\n",
    "        # Update the dataframe with text from the PDF file (or blank if not a PDF file)\n",
    "        my_sp.at[idx, 'PDFText'] = all_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
